import info.kgeorgiy.java.advanced.crawler.CachingDownloader;
import info.kgeorgiy.java.advanced.crawler.Crawler;
import info.kgeorgiy.java.advanced.crawler.Document;
import info.kgeorgiy.java.advanced.crawler.Downloader;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;


public class WebCrawler implements Crawler {
	
	ExecutorService executor;
	Downloader downloader;
	CachingDownloader cachDownloader;
	
//	downloaders — максимальное число одновременно загружаемых страниц;
//	extractors — максимальное число страниц, из которых извлекаются ссылки;
//	perHost — максимальное число страниц, одновременно загружаемых c одного хоста. 
//	Для опредения хоста следует использовать метод getHost класса URLUtils из тестов. 
	
	public WebCrawler(Downloader downloader, int downloaders, int extractors, int perHost) {
		System.err.println(downloaders + " " + extractors);
		executor = Executors.newFixedThreadPool(downloaders);
		this.downloader = downloader;
	}

	@Override
	public List<String> download(String url, int depth) throws IOException {
		if (depth == 0) {
			return new ArrayList<String>();
		}
		List<String> answer = new ArrayList<>();
		Set<String> ans = new HashSet<>();
		Document doc = downloader.download(url);
		List<String> links = doc.extractLinks();
		ans.addAll(links);
		for (int i = 0; i < links.size(); i++) {
			ans.addAll(download(links.get(i), depth - 1));
		}
		for (String s: ans) answer.add(s);
		return answer;
	}

	@Override
	public void close() {
		executor.shutdown();
	}
}
